{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree's are similar in nature to the binary tree discussed in classic computer science. \n",
    "A node of the decision tree contains a single input variable(X) and a split point on that variable. The ternimal nodes of the tree contain an output variable which is used to make a prediction.A tree can navigated with a new row of data following each branch with the splits until a final prediction is made. The difference between a binary tree and a Classification and Regression Trees(CART) is thier use of a cost function to determine the split points. A binary decision tree is a process of dividing up the input space though revursive binary splitting. For CART algorthims we split with the best cost( lowest cost because we minimize cost) is selected. All input varibles and all possible split points are evaluated and chosen in a manner based on the cost function. \n",
    "\n",
    "For Regression, The cost function that is minimized to choose split points is the sum squared error across all training samples that fall within the rctangle.\n",
    "\n",
    "For classification, A cos function called Gini is used which privdes an indication of hor pure the nodes are, where node purity refers to how mixed the training data assigned to each node is. \n",
    "\n",
    "Splitting continues until nodes contain a minimum number of training examples or a maximum tree depth is reached.\n",
    "\n",
    "As such, This class is broken down into 4 parts \n",
    "\n",
    "Gini Index \n",
    "Create split \n",
    "Build a Tree\n",
    "Make a  predciton\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini index evaluate splits in the dataset. a split involves one inpute attribute and one value for tha attribute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A split is comprised of an attribute in the datset and a value. The split is determined by the Gini score, splitting the dataset, and evauluating all splits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the tree, makes use of a binary tree node , and setting values for the tree's max depth , min sample of leafs and the min samples remaining prior to split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, Making predictions with a decision tree involves navigating the tree with the specifiically provide row of data.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "write up was inspired by Jason Brownlee, and his awesome tutoral:\n",
    "https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
